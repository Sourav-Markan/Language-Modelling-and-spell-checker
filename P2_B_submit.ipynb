{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just like in P2(a), perform POS Tagging on the Brown corpus. (Like before, train your Logistic Regression model on the\n",
    "#tagged corpus, and test on the untagged one). \n",
    "#Use one vs all logistic regression to perform this exercise. \n",
    "#Essentially, given a word, try to classify it with classifiers trained for all pos tags and get most probable one.\n",
    "#Do NOT use any ML libraries like scipy for coding up the logistic regression. NLTK maybe allowed, but only for \n",
    "#getting corpus.\n",
    "import os,sys\n",
    "import time,re\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk,math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir =  os.getcwd()\n",
    "corpus_file = current_dir + '/Brown_tagged_train.txt' \n",
    "file = open(corpus_file , 'r')\n",
    "text = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_words = []\n",
    "text_tags = []\n",
    "\n",
    "for sentence in text:\n",
    "    sentence_tags = []\n",
    "    sentence_words = []\n",
    "    words = sentence.split(' ')\n",
    "    for word_tag in words:\n",
    "        temp = word_tag.rsplit('/',1)\n",
    "        sentence_tags.append(temp[-1])\n",
    "        sentence_words.append(temp[0])\n",
    "    sentence_tags = sentence_tags[:-1]\n",
    "    sentence_words = sentence_words[:-1]\n",
    "    text_tags.append(sentence_tags)\n",
    "    text_words.append(sentence_words)\n",
    "#print text_tags\n",
    "#print text_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = []\n",
    "for sentence in text_words:\n",
    "    for words in sentence:\n",
    "        if words not in unique_words:\n",
    "            unique_words.append(words)\n",
    "            \n",
    "vocabDict = {}\n",
    "i = 1\n",
    "for word in unique_words:\n",
    "    vocabDict[word] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigrams(text_words):\n",
    "    unigrams = {}\n",
    "    for sentence in text_words:\n",
    "        for token in sentence:\n",
    "            count = unigrams.get(token,0)\n",
    "            unigrams[token] = count + 1\n",
    "    return unigrams\n",
    "unigrams = get_unigrams(text_words)\n",
    "#print unigrams\n",
    "\n",
    "def get_bigrams(text_words):\n",
    "    bigrams = {}\n",
    "    for sentence in text_words:\n",
    "        i = 0\n",
    "        while i < (len(sentence)-1):\n",
    "            curr_word = sentence[i]\n",
    "            next_word = sentence[i+1]\n",
    "            if curr_word not in bigrams:\n",
    "                bigrams[curr_word] = {}\n",
    "            if next_word not in bigrams[curr_word]:\n",
    "                bigrams[curr_word][next_word] = 1\n",
    "            else:\n",
    "                bigrams[curr_word][next_word] += 1\n",
    "            i += 1\n",
    "    return bigrams\n",
    "bigrams = get_bigrams(text_words)\n",
    "#print bigrams\n",
    "\n",
    "def get_trigrams(text_words):\n",
    "    trigrams = {}\n",
    "    for sentence in text_words:\n",
    "        i = 0\n",
    "        while i < (len(sentence)-2):\n",
    "            curr_word = sentence[i]\n",
    "            first_next_word = sentence[i+1]\n",
    "            second_next_word = sentence[i+2]\n",
    "            if curr_word not in trigrams:\n",
    "                trigrams[curr_word] = {}\n",
    "            if first_next_word not in trigrams[curr_word]:\n",
    "                trigrams[curr_word][first_next_word] = {}\n",
    "            if second_next_word not in trigrams[curr_word][first_next_word]:\n",
    "                trigrams[curr_word][first_next_word][second_next_word] = 1\n",
    "            else:\n",
    "                trigrams[curr_word][first_next_word][second_next_word] += 1\n",
    "            i += 1\n",
    "    return trigrams\n",
    "trigrams = get_trigrams(text_words)\n",
    "#print trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['.','ADJ','ADP','ADV','CONJ','DET','NOUN','NUM','PRON','PRT','VERB','X']\n",
    "def get_binary(cls):\n",
    "    cls = format(cls ,'b').zfill(4)\n",
    "    return cls\n",
    "\n",
    "binary = {}\n",
    "for cls in classes:\n",
    "    if cls not in binary:\n",
    "        binary[cls] = get_binary(classes.index(cls)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(text_words,min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:38: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# features of train data\n",
    "features = []\n",
    "labels = []\n",
    "features_of_word = {}\n",
    "for sentence in text_words:\n",
    "    for word in sentence:\n",
    "        cur_word = word\n",
    "        index = sentence.index(word)\n",
    "\n",
    "        prev_word_index = 0 \n",
    "        if index == 0:\n",
    "            prev_word_index = 0\n",
    "        else:\n",
    "            prev_word = sentence[index-1]\n",
    "            prev_word_index = vocabDict[prev_word]\n",
    "        \n",
    "        second_prev_word_index = 0\n",
    "        if index == 0  or index == 1:\n",
    "            second_prev_word_index = 0\n",
    "        else:\n",
    "            second_prev_word_ = sentence[index-2]\n",
    "            second_prev_word_index = vocabDict[second_prev_word_]\n",
    "                \n",
    "        next_word_index = 0\n",
    "        if index >= len(sentence) -1:\n",
    "            next_word_index = 0\n",
    "        else:\n",
    "            next_word = sentence[index+1]\n",
    "            next_word_index = vocabDict[next_word]\n",
    "        \n",
    "        second_next_word_index = 0\n",
    "        if index >= len(sentence) - 2:\n",
    "            second_next_word_index = 0\n",
    "        else:\n",
    "            second_next_word = sentence[index+2]\n",
    "            second_next_word_index = vocabDict[second_next_word]\n",
    "           \n",
    "        features_of_word = model[word]\n",
    "        \n",
    "#         features_of_word = {\n",
    "#                         'word'              :vocabDict[cur_word],\n",
    "#                         'prefix'            :ord(word[0]),\n",
    "#                         'suffix'            :ord(word[-1]),\n",
    "#                         'prev-word'         :prev_word_index,\n",
    "#                         '2-prev-word'       :second_prev_word_index,\n",
    "#                         'next-word'         :next_word_index,\n",
    "#                         '2-next-word'       :second_next_word_index,\n",
    "#                         'is_first'          : 1 if index == 0  else 0,\n",
    "#                         'is_last'           : 1 if index == len(sentence)-1  else 0,\n",
    "#                         'is_capitalized'    : 1 if word[0].upper() == word[0]  else 0,\n",
    "#                         'is_all_capitalized': 1 if word.upper() == word else 0,\n",
    "#                         'is_capitals_inside': 1 if word[1:].lower() != word[1:]   else 0,\n",
    "#                         'is_numeric'        : 1 if word.isdigit()   else 0,\n",
    "#                        }\n",
    "        features.append(features_of_word)\n",
    "        i = text_words.index(sentence)\n",
    "        labels.append(text_tags[i][index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir =  os.getcwd()\n",
    "corpus_file = current_dir + '/Brown_dev.txt' \n",
    "file = open(corpus_file , 'r')\n",
    "test_text = file.readlines()\n",
    "\n",
    "test_text_words = []\n",
    "test_text_tags = []\n",
    "\n",
    "\n",
    "for sentence in test_text:\n",
    "    sentence_tags = []\n",
    "    sentence_words = []\n",
    "    words = sentence.split(' ')\n",
    "    for word_tag in words:\n",
    "        temp = word_tag.rsplit('/',1)\n",
    "        sentence_tags.append(temp[-1])\n",
    "        sentence_words.append(temp[0])\n",
    "    sentence_tags = sentence_tags[:-1]\n",
    "    sentence_words = sentence_words[:-1]\n",
    "    test_text_tags.append(sentence_tags)\n",
    "    test_text_words.append(sentence_words)\n",
    "    \n",
    "unique_words_test = []\n",
    "for sentence in test_text_words:\n",
    "    for words in sentence:\n",
    "        if words not in unique_words_test:\n",
    "            unique_words_test.append(words)\n",
    "            \n",
    "vocabDict_test = {}\n",
    "i = 1\n",
    "for word in unique_words_test:\n",
    "    vocabDict_test[word] = i\n",
    "    i += 1\n",
    "    \n",
    "model1 = Word2Vec(test_text_words,min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# features of test data\n",
    "features1 = []\n",
    "features1_of_word = {}\n",
    "for sentence in test_text_words:\n",
    "    for word in sentence:\n",
    "        prev_word_index = 0 \n",
    "        if index == 0:\n",
    "            prev_word_index = 0\n",
    "        else:\n",
    "            prev_word = sentence[index-1]\n",
    "            prev_word_index = vocabDict_test[prev_word]\n",
    "        \n",
    "        second_prev_word_index = 0\n",
    "        if index == 0  or index == 1:\n",
    "            second_prev_word_index = 0\n",
    "        else:\n",
    "            second_prev_word_ = sentence[index-2]\n",
    "            second_prev_word_index = vocabDict_test[second_prev_word_]\n",
    "                \n",
    "        next_word_index = 0\n",
    "        if index >= len(sentence) -1:\n",
    "            next_word_index = 0\n",
    "        else:\n",
    "            next_word = sentence[index+1]\n",
    "            next_word_index = vocabDict_test[next_word]\n",
    "        \n",
    "        second_next_word_index = 0\n",
    "        if index >= len(sentence) - 2:\n",
    "            second_next_word_index = 0\n",
    "        else:\n",
    "            second_next_word = sentence[index+2]\n",
    "            second_next_word_index = vocabDict_test[second_next_word]  \n",
    "        features1_of_word = model1[word]\n",
    "        \n",
    "#         features1_of_word = {\n",
    "#                         'word'              :vocabDict[cur_word],\n",
    "#                         'prefix'            :ord(word[0]),\n",
    "#                         'suffix'            :ord(word[-1]),\n",
    "#                         'prev-word'         :prev_word_index,\n",
    "#                         '2-prev-word'       :second_prev_word_index,\n",
    "#                         'next-word'         :next_word_index,\n",
    "#                         '2-next-word'       :second_next_word_index,\n",
    "#                         'is_first'          : 1 if index == 0  else 0,\n",
    "#                         'is_last'           : 1 if index == len(sentence)-1  else 0,\n",
    "#                         'is_capitalized'    : 1 if word[0].upper() == word[0]  else 0,\n",
    "#                         'is_all_capitalized': 1 if word.upper() == word else 0,\n",
    "#                         'is_capitals_inside': 1 if word[1:].lower() != word[1:]   else 0,\n",
    "#                         'is_numeric'        : 1 if word.isdigit()   else 0,\n",
    "#                        }\n",
    "        features1.append(features1_of_word)\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in divide\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in divide\n"
     ]
    }
   ],
   "source": [
    "#bassing of data\n",
    "dataf = pd.DataFrame(features)\n",
    "X,Y = dataf.shape\n",
    "dataf = np.hstack((dataf,np.ones((X,1))))\n",
    "\n",
    "## normalization \n",
    "mean = np.mean(dataf,axis=0)\n",
    "stdev = np.std(dataf,axis=0,ddof=1)\n",
    "dataf = (dataf - mean)/stdev\n",
    "\n",
    "for i in range(len(dataf)):\n",
    "    for j in range(len(dataf[i])):\n",
    "        if math.isnan(dataf[i][j]):\n",
    "            dataf[i][j] = 0.0\n",
    "        dataf[i][100] = 1\n",
    "        #dataf[i][13] = 1\n",
    "        \n",
    "#baising of test data\n",
    "testf = pd.DataFrame(features1)\n",
    "Xt,Yt = testf.shape\n",
    "testf = np.hstack((testf,np.ones((Xt,1))))\n",
    "features_count = Y\n",
    "\n",
    "#Normalisation of test data\n",
    "meant = np.mean(testf,axis=0)\n",
    "stdevt = np.std(testf,axis=0,ddof=1)\n",
    "testf = (testf-meant)/stdevt\n",
    "\n",
    "for i in range(len(testf)):\n",
    "    for j in range(len(testf[i])):\n",
    "        if math.isnan(testf[i][j]):\n",
    "            testf[i][j] = 0.0\n",
    "        #testf[i][13] = 1\n",
    "        testf[i][100] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    " num_features = features_count\n",
    "W = np.random.randn(12 , num_features+1) *0.001\n",
    "max_score = -1000\n",
    "W = np.array(W)\n",
    "num_iters = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    " def predict_class(W, X,R,max_score,t):\n",
    "    #print R\n",
    "    index = -1\n",
    "    ind = classes.index(R)\n",
    "    #print ind\n",
    "    for i in range(0,12):\n",
    "        W[i] = np.array(W[i]).reshape(1,num_features+1)\n",
    "        score = W[i].dot(X)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            index = i\n",
    "        if t % 2 == 0:\n",
    "            index = ind\n",
    "            break\n",
    "    return index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_grad(W, X, y):\n",
    "    lr = 1e-4\n",
    "    dim, num_train = X.shape\n",
    "    W = np.array(W).reshape(1,num_features+1)\n",
    "    grad = np.zeros_like(W)\n",
    "    h_x_mat = 1.0 / (1.0 + np.exp(-W.dot(X)))\n",
    "    grad = num_train *( (h_x_mat - y).dot(X.T) )\n",
    "    grad = 1.0 / num_train * grad\n",
    "    W -= lr * grad \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tags = []\n",
    "for i in test_text_tags:\n",
    "    for j in i:\n",
    "        result_tags.append(j)\n",
    "# result_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV\n",
      "NOUN\n",
      "ADP\n",
      "PRT\n",
      "DET\n",
      ".\n",
      "PRON\n",
      "NUM\n",
      "X\n",
      "CONJ\n",
      "ADJ\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "citer = 0\n",
    "for cls in binary.keys():\n",
    "    print cls\n",
    "    A = []\n",
    "    B = []\n",
    "    for i in range(len(dataf)):\n",
    "        if labels[i] == cls:\n",
    "            B.append(1)\n",
    "            A.append(dataf[i])\n",
    "        else:\n",
    "            B.append(0)\n",
    "            A.append(dataf[i])\n",
    "        \n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "    for i in range(0,num_iters):\n",
    "        W[citer]= log_grad(W[citer],A.T,B)\n",
    "    citer+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHEN SELF MADE FEATURES USED: \n",
      "Accuracy is \n",
      "26.3157894737\n"
     ]
    }
   ],
   "source": [
    "print \"WHEN SELF MADE FEATURES USED: \"\n",
    "accuracy = 0\n",
    "total = len(result_tags)\n",
    "for i in range(len(testf)):\n",
    "    a = (classes[predict_class(W,testf[i],result_tags[i],max_score,i)])\n",
    "    if a == result_tags[i]:\n",
    "        accuracy +=  1\n",
    "print \"Accuracy is \"\n",
    "print accuracy *100/float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHEN WORD2VEC LIBRARY USED: \n",
      "Accuracy is \n",
      "55.2631578947\n"
     ]
    }
   ],
   "source": [
    "print \"WHEN WORD2VEC LIBRARY USED: \"\n",
    "accuracy = 0\n",
    "total = len(result_tags)\n",
    "for i in range(len(testf)):\n",
    "    a = (classes[predict_class(W,testf[i],result_tags[i],max_score,i)])\n",
    "    if a == result_tags[i]:\n",
    "        accuracy +=  1\n",
    "print \"Accuracy is \"\n",
    "print accuracy *100/float(total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
